# Projectdocumentation

## 22.12.2018

Setting up the process of getting data from homepage, open it,
count words and exclude special characters from counting process.

Following modules are used in this case:

  from urllib               import request
  from collections          import Counter
  from nltk                 import word_tokenize, FreqDist
  from wordcloud            import WordCloud
  from nltk.corpus          import stopwords

  import numpy              as np
  import matplotlib.pyplot  as plt
  import random
  import nltk
  import requests

## 26.12.2018

First error occured here:
"Failed loading english.pickle with nltk.data.load"

solution found on stackoverflow for that problem:
  1. Got to a prompt Shell
  2. Write the following Code:
  3. import nltk.data
     nltk.download()
  4. Press 'Enter'
  5. The installation window appears.
  6. Go to the 'Models' tab and select 'punkt' from the identifier column.
  7. Then click 'Download' and it will install the necessary files.

## 27.12.2018

Reading pandas documentation

Installing finally the following libraries:
Simply copied the following instructions in the terminal and execute

  conda install -c anaconda nltk 
  conda install -c ulmo urllib3
  conda install -c conda-forge wordcloud
  conda install -c anaconda numpy
  conda install -c conda-forge matplotlib

## 13.02.2019

Creating a virtual environment and upload the first try in a repository on my github account.

Following sites were very useful:

  https://ipython.readthedocs.io/en/stable(install(kernel_install.html
  https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/

## 17.02.2019

Structuring the whole writing of "README.md" and "documentation.txt"
Strucuturing means what and how to wirte.

## 20.02.2019

Adding Output for specific letters in words.

# 22.02.2018

Adding more specifications and doing some testing.

Trying to do some debugging in the file called "program_debugging.ipynb".
The program used for debugging called pixiedebugger.













